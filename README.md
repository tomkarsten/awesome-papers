# :books: Awesome Papers for Data/ML Scientists

A curated list of papers that may be of interest to Data Scientists and Machine Learning students and professionals:

**NOV-2024**
* [A Survey on Data Synthesis and Augmentation for Large Language Models (2024)](https://arxiv.org/abs/2410.12896)
* [The Prompt Report: A Systematic Survey of Prompting Techniques (2024)](https://trigaten.github.io/Prompt_Survey_Site/)
  
**JUN-2024**
* [An Introduction to Vision-Language Modeling (2024)](https://arxiv.org/abs/2405.17247)
* [Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land (2024)](https://arxiv.org/abs/2404.17625)
  
**MAR-2024**
* [Stable Diffusion 3 (2024)](https://stability.ai/news/stable-diffusion-3-research-paper)
* 
**FEB-2024**
* [Large Language Models: A Survey (2024)](https://arxiv.org/abs/2402.06196)
* [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context (2024)](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)
  
**JAN-2024**
* [Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM (2024)](https://arxiv.org/abs/2401.02994)
* [Leveraging Large Language Models for NLG Evaluation: A Survey (2024)](https://arxiv.org/abs/2401.07103)
* [Foundations of Vector Retrieval (2024)](https://arxiv.org/abs/2401.09350)
  
**DEC-2023**
* [An In-depth Look at Gemini's Language Abilities (2023)](https://arxiv.org/abs/2312.11444)
* [Gemini: A Family of Highly Capable Multimodal Models (2023)](https://paperswithcode.com/paper/gemini-a-family-of-highly-capable-multimodal)
* [ Mamba: Linear-Time Sequence Modeling with Selective State Spaces (2023)](https://arxiv.org/abs/2312.00752)
* [Data Management For Large Language Models: A Survey (2023)](https://arxiv.org/abs/2312.01700)
* [The Efficiency Spectrum of Large Language Models: An Algorithmic Survey (2023)](https://arxiv.org/abs/2312.00678)
*   
**NOV-2023**
* [Have we built machines that think like people? (2023)](https://arxiv.org/abs/2311.16093)
* [ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up? (2023)](https://arxiv.org/abs/2311.16989)

**OCT-2023**
* [Large Language Model Alignment: A Survey (2023)](https://arxiv.org/abs/2309.15025)

**SEP-2023**
* [Applications of Deep Neural Networks with Keras (2022)](https://arxiv.org/abs/2009.05673)
* [The Modern Mathematics of Deep Learning (2023)](https://arxiv.org/abs/2105.04026)
* [Multimodal Deep Learning (2023)](https://arxiv.org/abs/2301.04856)
* [To SMOTE, or not to SMOTE? (2023)](https://arxiv.org/abs/2201.08528)
* [Instruction Tuning for Large Language Models: A Survey (2023)](https://arxiv.org/abs/2308.10792)
* [Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities (2023)](https://arxiv.org/abs/2308.12833)
* 
**AUG-2023**
* [A Survey on Multimodal Large Language Models (2023)](https://arxiv.org/abs/2306.13549)
* [Efficient Guided Generation for Large Language Models (2023)](https://arxiv.org/abs/2307.09702)
* [From Pretraining Data to Language Models to Downstream Tasks Tracking the Trails of Political Biases Leading to Unfair NLP Models (2023)](https://arxiv.org/abs/2305.08283)
* [Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment (2023)](https://arxiv.org/abs/2308.05374)
  
**JULY-2023**
* [Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better (2021)](https://arxiv.org/abs/2106.08962)
* [Foundational Aligning Large Language Models with Human: A Survey (2023)](https://arxiv.org/abs/2307.12966v1)
* [Foundational Models Defining a New Era in Vision: A Survey and Outlook (2023)](https://arxiv.org/abs/2307.13721v1)
* [Challenges and Applications of Large Language Models (2023)](https://arxiv.org/abs/2307.10169)
* [How is ChatGPT's behavior changing over time? (2023)](https://arxiv.org/abs/2307.09009)
* [A Survey on Evaluation of Large Language Models (2023)](https://arxiv.org/abs/2307.03109)
  
**JUNE-2023**
* [FinGPT: Open-Source Financial Large Language Models (2023)](https://arxiv.org/abs/2306.06031)
* [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources (2023)](https://arxiv.org/abs/2306.04751)
* [A Simple and Effective Pruning Approach for Large Language Models (2023)](https://arxiv.org/abs/2306.11695)
* [Reasoning with Language Model Prompting: A Survey (2023)](https://arxiv.org/abs/2212.09597)
* [All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network (2023)](https://arxiv.org/abs/2303.14957)

 
**JUNE-2023**
* [An Overview of Catastrophic AI Risks (2023)](https://arxiv.org/abs/2306.12001v1)
* [A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks (2023)](https://arxiv.org/abs/2306.07303)
* [The Impact of Positional Encoding on Length Generalization in Transformers (2023)](https://arxiv.org/abs/2305.19466)
* [CodeTF: One-stop Transformer Library for State-of-the-art Code LLM (2023)](https://arxiv.org/abs/2306.00029)
* [Everyone wants to do the model work, not the data work: Data Cascades in High-Stakes AI (2021)](https://research.google/pubs/pub49953/)
* [Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning (2019)](https://arxiv.org/abs/1811.12808)
* [Evaluating the Quality of Machine Learning Explanations (2020)](https://www.mdpi.com/2079-9292/10/5/593/pdf)
* [Evaluation of statistical and machine learning models for time series prediction (2019)](https://www.researchgate.net/profile/Antonio-Parmezan/publication/330742498_Evaluation_of_statistical_and_machine_learning_models_for_time_series_prediction_Identifying_the_state-of-the-art_and_the_best_conditions_for_the_use_of_each_model/links/5fbc07eda6fdcc6cc65e0d18/Evaluation-of-statistical-and-machine-learning-models-for-time-series-prediction-Identifying-the-state-of-the-art-and-the-best-conditions-for-the-use-of-each-model.pdf)

**MAY-2023**

* [A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models (2023)](https://arxiv.org/abs/2305.12544)
* [The False Promise of Imitating Proprietary LLMs (2023)](https://arxiv.org/abs/2305.15717)
* [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training (2023)](https://arxiv.org/abs/2305.14342)
* [Trustworthy AI: From Principles to Practices (2023)](https://arxiv.org/abs/2110.01167)
* [Machine Learning Testing: Survey, Landscapes and Horizons (2019)](https://arxiv.org/abs/1906.10742)
* [BIML Interactive Machine Learning Risk Framework (2023)](https://berryvilleiml.com/interactive/)
* [A Survey on the Explainability of Supervised Machine Learning (2020)](https://arxiv.org/abs/2011.07876)
* [Cramming: Training a Language Model on a Single GPU in One Day (2023)](https://arxiv.org/abs/2212.14034)
* [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks (2023)](https://huggingface.co/papers/2305.14201)
* [A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models (2023)](https://arxiv.org/abs/2305.12544)
* [Excuse me, do you have a moment to talk about version control? (2017)](https://peerj.com/preprints/3159.pdf)
* [Good Enough Practices in Scientific Computing (2016)](https://arxiv.org/abs/1609.00037)
* [RWKV: Reinventing RNNs for the Transformer Era (2023)](https://arxiv.org/abs/2305.13048)
* [Scaling Speech Technology to 1,000+ Languages (2023)](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/)
* [Any-to-Any Generation via Composable Diffusion (2023)](https://arxiv.org/abs/2305.11846)
